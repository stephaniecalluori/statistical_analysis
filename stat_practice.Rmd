---
title: "Statistical Learning"
output: github_document
---

# Load packages and set seed
```{r packages, message = FALSE}
library(tidyverse)
library(glmnet)

set.seed(11)

```

```{r setup, message = FALSE, echo = FALSE, results = FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "right"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Lasso
```{r}
bwt_df = 
  read_csv("data/birthweight.csv") |> 
  janitor::clean_names() |>
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(
      frace, "white" = "1", "black" = "2", "asian" = "3", 
      "puerto rican" = "4", "other" = "8"),
    malform = as.logical(malform),
    mrace = as.factor(mrace),
    mrace = fct_recode(
      mrace, "white" = "1", "black" = "2", "asian" = "3", 
      "puerto rican" = "4")) |> 
  sample_n(200)
```

get predictors and outcome
instead of specifying the name of all the variables we just want to provide the values to glmnet and tell glmnet to put it all in there; so our design matrix will have the values of all of the variables
-1 is just us removing the first column intercept
```{r}
x = model.matrix(bwt ~ ., bwt_df)[, -1]

y = bwt_df |> pull(bwt)

```
 
now we need to fit lasso
creating grid of lambda values
we're specifying lambdas but you don't have to; glmnet has defaults

run lasso
then run cross validation
```{r}
lambda = 10^(seq(3, -2, -0.1))

lasso_fit <-
  glmnet(x, y, lambda = lambda)

lasso_cv <- 
  cv.glmnet(x, y, lambda = lambda)
```

let's look at lasso results!
at step 15 (lambda value of 39.8), these are our variables and their estimates
for each step see what's showing up in your model
if categorical factor, the way we're doing it now lasso can say that some levels of a factor are important and some are not
```{r}
lasso_fit |> 
  broom::tidy() |> 
  filter(step == 15)
```

let's visualize!
when i have missing lambda valye make estimate 0 (meaning that variable want's included in that model)
filling in regression coefficients where they should be 0
```{r}
lasso_fit |> 
  broom::tidy() |> 
  filter(term != "(Intercept)") |> 
  complete(term, lambda, fill = list(estimate = 0)) |> 
  ggplot(aes(x = log(lambda, 10), y = estimate, color = term, group = term)) +
  geom_path()
```
far left everything incuded in model; far right nothing included in model, all coefficients go to zero

```{r}
lambda_opt = lasso_cv$lambda.min
log(10, 10)

lasso_fit |> 
  broom::tidy() |> 
  filter(lambda == 10)
```

Show CV results
```{r}
lasso_cv |> 
  broom::tidy() |> 
  ggplot(aes(x = log(lambda, 10), y = estimate)) +
  geom_point()
  
```

# K-means clustering

```{r}
library(palmerpenguins)

data("penguins")

penguins |> 
  ggplot(aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +
  geom_point()

penguins |> 
  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
  geom_point()
```

can we recreate this via clustering?

```{r}
penguins = penguins |> 
  select(species, bill_length_mm, flipper_length_mm) |> 
  drop_na()

kmeans_fit = penguins |> 
  select(-species) |>
  scale() |> 
  kmeans(centers = 3)

penguins |> 
  broom::augment(kmeans_fit, data = _)

```
make it so it tells you want the cluster assignment was

```{r}
penguins |> 
  broom::augment(kmeans_fit, data = _) |> 
  ggplot(aes(x = bill_length_mm, y = flipper_length_mm, color = .cluster)) +
  geom_point()
```
since we scaled scale(); grouping looks better



